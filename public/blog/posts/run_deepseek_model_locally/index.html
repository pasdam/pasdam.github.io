<!doctype html><html lang=en><head><meta charset=utf-8><title>Run your own private AI assistant locally with Chat-UI and Deepseek on Docker &ndash; /dev/null
</title><link href="https://fonts.googleapis.com/css?family=Work+Sans:600|Quattrocento+Sans:400,400i,700" rel=stylesheet type=text/css><link rel=icon href=https://pasdam.github.io/favicon.svg><link rel=stylesheet href=https://pasdam.github.io/global.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><meta name=description content="Guide on how to spin up ChatUI, using Deepseek model from Ollama library, on docker"><meta name=viewport content="width=device-width,initial-scale=1"><script async src="https://www.googletagmanager.com/gtag/js?id=G-H52BY7123V"></script><script async src=https://pasdam.github.io/copy-code-button.min.js onload='setupCopyButtons("copy","copied!")'></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H52BY7123V",{cookie_domain:"pasdam.github.io",cookie_flags:"SameSite=None;Secure"})</script><link href=https://cdn.jsdelivr.net/npm/shareon@2/dist/shareon.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/shareon@2/dist/shareon.iife.js defer init></script></head><body><header id=page-header class=bg-green><nav><div class=logo-container><h1 class=logo><a href=/>/dev/null - Blog</a></h1></div><script>function toggleClassNameById(e,t){const n=document.getElementById(e).classList;n.contains(t)?n.remove(t):n.add(t)}</script><button data-collapse-toggle=mega-menu type=button class=mega-menu-toggle aria-controls=mega-menu aria-expanded=false onclick='toggleClassNameById("mega-menu","hidden")'>
<span class=sr-only>Open main menu</span><svg class="icon" aria-hidden="true" fill="none" viewBox="0 0 17 14"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"/></svg></button><div id=mega-menu><ul><li><a href=/><span>Home</span></a></li><li><a aria-current=true class=ancestor href=/blog/posts/><span>Blog</span></a></li><li><a href=https://github.com/pasdam target=_blank><i class="fa-brands fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/pasdam target=_blank><i class="fa-brands fa-linkedin"></i></a></li></ul></div></nav></header><div id=main><article class=post-body><h1 class=text-green>Run your own private AI assistant locally with Chat-UI and Deepseek on Docker</h1><p class=date-read-time>February 1, 2025 &mdash; 4 Min Read</p><div class=tags><div class=tag><a href=/tags/ai/ class=test>AI</a></div><div class=tag><a href=/tags/chatui/ class=test>ChatUI</a></div><div class=tag><a href=/tags/deepseek/ class=test>Deepseek</a></div><div class=tag><a href=/tags/docker/ class=test>Docker</a></div><div class=tag><a href=/tags/llm/ class=test>LLM</a></div><div class=tag><a href=/tags/ollama/ class=test>Ollama</a></div><div class=tag><a href=/tags/tutorial/ class=test>Tutorial</a></div></div><hr><nav id=TableOfContents><ul><li><a href=#tldr>TLDR</a></li><li><a href=#why-run-deepseek-locally>Why Run DeepSeek Locally?</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#code>Code</a></li><li><a href=#access-chat-ui>Access Chat-UI</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><hr><div class=content><p>Want to harness the power of
<a href=https://ollama.com/library/deepseek-r1:1.5b>Deepseek</a> LLM without relying on
external APIs or worrying about data privacy? Running it locally is the answer!
This guide will walk you through setting up all you need on your own machine
using the user-friendly <a href=https://github.com/huggingface/chat-ui>Chat UI</a> and
the containerization magic of Docker. Get ready to chat with your very own
private AI assistant!</p><h2 id=tldr>TLDR</h2><p>Check out my <a href=https://github.com/pasdam/docker-ollama-deepseek-chatui>repo</a> and
then run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make compose-up
</span></span></code></pre></div><p>and wait for the images and the model to be downloaded, it might take some
minutes.</p><h2 id=why-run-deepseek-locally>Why Run DeepSeek Locally?</h2><p>There might be few reasons why one would want to use a local assistan rather
than using one of the many services available on the internet:</p><ul><li><em>privacy</em>: keep your conversations and data completely private and avoid
sending sensitive information to external servers;</li><li><em>cost</em>: no API usage fees or rate limit, you can use it as much as you want;</li><li><em>offline access</em>: use the assistant even without an internet connection;</li><li><em>customization</em>: fine-tune and customize the model to your specific needs.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Please refer to the official documentations to install all the required tools:</p><ul><li><a href=https://docs.docker.com/get-started/get-docker/>docker</a>.</li></ul><p>You&rsquo;ll also need sufficient resources on the machine you&rsquo;re executing the model
on: LLM models can be resource-intensive. Ensure your machine has enough RAM (at
least 16GB recommended, more for larger models) and disk space (depending on the
model size).</p><p>DeepSeek offers various models, start with a smaller one if you&rsquo;re limited on
resources. For the purpose of this tutorial we are using
<a href=https://ollama.com/library/deepseek-r1:1.5b>deepseek-r1:1.5b</a>, which is relatively
performant and small, but other verions are available with different number of
parameters.</p><p>You can find more models to test in the
<a href=https://ollama.com/library>Ollama library</a>.</p><h2 id=limitations>Limitations</h2><p>A dedicated GPU would be highly recommended for reasonable performance, but
unfortunately, according to the
<a href=https://docs.docker.com/desktop/features/gpu/>docker official documentation</a>:</p><blockquote><p>Currently GPU support in Docker Desktop is only available on Windows with the
WSL2 backend.</p></blockquote><p>If you encounter memory or resource errors, try using a smaller model,
increasing the resources allocated to Docker, or stopping other running
containers.</p><h2 id=code>Code</h2><p>Here&rsquo;s the docker compose configuration to run all the needed components:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ollama</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>ollama/ollama:0.5.7</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>./ollama/data:/root/.ollama</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ollama-init</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>alpine/curl:8.11.1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>: -<span style=color:#ae81ff>X POST http://ollama:11434/api/pull -d &#39;{&#34;model&#34;:&#34;deepseek-r1:1.5b&#34;}&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>depends_on</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>ollama</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>chat-ui</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>ghcr.io/huggingface/chat-ui-db:sha-31344ad</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>./chat-ui/db:/data/db</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>MODELS</span>: &gt;<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;name&#34;: &#34;Ollama DeepSeek&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;chatPromptTemplate&#34;: &#34;&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;parameters&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              &#34;stop&#34;: [&#34;&lt;/s&gt;&#34;]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            },
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;endpoints&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                &#34;type&#34;: &#34;ollama&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                &#34;url&#34; : &#34;http://ollama:11434&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                &#34;ollamaName&#34; : &#34;deepseek-r1:1.5b&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        ]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>3000</span>:<span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>depends_on</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>ollama-init</span>
</span></span></code></pre></div><p>The first service, <code>ollama</code>, is the <a href=https://ollama.com/>model manager</a>. For
this we need to attach a volume, <code>.ollama/data</code>, where the model(s) will be
saved.</p><p><code>ollama-init</code> is a simple container that will simply use the
<a href=https://github.com/ollama/ollama/blob/main/docs/api.md#pull-a-model>Ollama APIs</a>
to request <code>ollama</code> service to download the required model. This container is
not strictly required if you prefer to perform the downlaod manually separately,
but just remember to save the file in <code>.ollama/data</code>, the model folder we
mounted as volume in <code>ollama</code>.</p><p>To be able to ask question we can use the
<a href=https://github.com/huggingface/chat-ui>chat web UI</a> provided by
<a href=https://huggingface.co/chat>HuggingFace</a>, in the <code>chat-ui</code> service.</p><p>First thing we need here is to mount the folder where search data can be stored,
<code>./chat-ui/db</code>. Then we need to configure the models list, that only contains
one for this tutorial. In particular we need to make sure to match the value in
<code>ollamaName</code> to the model we download in <code>ollama-init</code>.</p><h2 id=access-chat-ui>Access Chat-UI</h2><p>You can start the services as per usual with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker compose up -d
</span></span></code></pre></div><p>Then you can open your web browser and go to
<a href=http://localhost:3000>http://localhost:3000</a> and ask questions to your
personal assistant.</p><p>Note that depending on the hardware and the chosen model, it might take some
time to provide an answer, so be patient ;).</p><h2 id=conclusion>Conclusion</h2><p>By setting up Ollama locally using Docker, you can experiment with the DeepSeek
model without relying on cloud services. This approach offers control over
resources and allows you to test different configurations. However, it is
limited by your hardware capabilities during training and execution.</p></div></article><hr><div class=related><h3>See Also:</h3><ul><li><a href=/blog/posts/deploy_openfaas_to_minikube/>Deploy OpenFaas to minikube</a></li><li><a href=/blog/posts/gatsbyjs_add_diqus_comments/>Add Disqus comments to a GatsbyJS static blog</a></li><li><a href=/blog/posts/go_library_for_c_cpp_applications/>Use Go code in c/c++ applications</a></li><li><a href=/blog/posts/ssh_multiple_keys_config/>How to use different SSH keys</a></li><li><a href=/blog/posts/datadog_agent_docker/>Install DataDog Agent as a docker container</a></li></ul></div><hr><div class=socialshare><p>Share this</p><div class=shareon><a class=facebook title="Share on Facebook"></a><a class=linkedin title="Share on Linkedin"></a><a class=mastodon title="Share on Mastodon"></a><a class=messenger data-fb-app-id=0123456789012345 title="Share on Messenger"></a><a class=pinterest title="Share on Pinterest"></a><a class=pocket title="Share on Pocket"></a><a class=reddit title="Share on Reddit"></a><a class=telegram title="Share on Telegram"></a><a class=twitter title="Share on Twitter"></a><a class=vkontakte title="Share on Vkontakte"></a><a class=whatsapp title="Share on Whatsapp"></a><a class=copy-url title="Copy link"></a><a class=email title="Share by Email"></a><a class=print title=Print></a></div></div><hr><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//pasdam-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a>
<button class=hidden id=to-top-button onclick=goToTop() title="Go To Top">
<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="chevron-circle-up" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm231-113.9L103.5 277.6c-9.4 9.4-9.4 24.6.0 33.9l17 17c9.4 9.4 24.6 9.4 33.9.0L256 226.9l101.6 101.6c9.4 9.4 24.6 9.4 33.9.0l17-17c9.4-9.4 9.4-24.6.0-33.9L273 142.1c-9.4-9.4-24.6-9.4-34 0z"/></svg>
</button>
<script async>const offset=100;var toTopButton=document.getElementById("to-top-button");window.onscroll=function(){document.body.scrollTop>offset||document.documentElement.scrollTop>offset?toTopButton.classList.remove("hidden"):toTopButton.classList.add("hidden")};function goToTop(){window.scrollTo({top:0,behavior:"smooth"})}</script></body></html>